# Projet WuxiaWorld Scraper & eBook Generator

## ğŸ“‹ Membres du groupe
- [Nom du membre 1]
- [Nom du membre 2]
- [Nom du membre 3]

## ğŸ“– Description du projet

Ce projet est un scraper avancÃ© pour wuxiaworld.com qui permet de :
- Rechercher des livres par titre avec autocomplÃ©tion
- Scraper les chapitres de maniÃ¨re asynchrone et optimisÃ©e
- GÃ©nÃ©rer des ebooks au format EPUB avec une structure complÃ¨te
- Interface web moderne et intuitive
- Base de donnÃ©es pour stocker les donnÃ©es en temps rÃ©el
- Pipeline de traitement optimisÃ©

## ğŸ—ï¸ Structure du projet

```
groupe-webscrapy/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # API Flask
â”‚   â”œâ”€â”€ database.py            # Configuration base de donnÃ©es
â”‚   â”œâ”€â”€ models.py              # ModÃ¨les de donnÃ©es
â”‚   â””â”€â”€ requirements.txt       # DÃ©pendances backend
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ wuxia_scraper/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ wuxia_spider.py
â”‚   â”‚   â”œâ”€â”€ items.py           # DÃ©finition des items
â”‚   â”‚   â”œâ”€â”€ pipelines.py       # Pipeline de traitement
â”‚   â”‚   â”œâ”€â”€ settings.py        # Configuration Scrapy
â”‚   â”‚   â””â”€â”€ middlewares.py     # Middlewares personnalisÃ©s
â”‚   â”œâ”€â”€ epub_generator.py      # GÃ©nÃ©rateur EPUB
â”‚   â”œâ”€â”€ scrapy.cfg
â”‚   â””â”€â”€ requirements.txt       # DÃ©pendances scraper
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ styles.css
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ assets/
â””â”€â”€ docker-compose.yml         # Configuration Docker

```

## ğŸš€ Instructions d'exÃ©cution

### PrÃ©requis
- Python 3.8+
- PostgreSQL ou MongoDB
- Node.js (optionnel pour le dÃ©veloppement frontend)

### 1. Installation des dÃ©pendances

#### Backend
```bash
cd backend
pip install -r requirements.txt
```

#### Scraper
```bash
cd scraper
pip install -r requirements.txt
```

### 2. Configuration de la base de donnÃ©es

1. CrÃ©er un cluster MongoDB (par exemple via MongoDB Atlas)
2. Configurer les variables d'environnement dans `.env`

### 3. Lancement des composants

#### Backend API
```bash
cd backend
python app.py
```

#### Interface Web
Ouvrir `frontend/index.html` dans un navigateur ou servir avec un serveur web local.

#### Scraper (manuel)
```bash
cd scraper
scrapy crawl wuxia -a search_query="titre_du_livre"
```

## ğŸ”§ Configuration

Les paramÃ¨tres de configuration se trouvent dans :
- `backend/.env` : Configuration base de donnÃ©es et API
- `scraper/wuxia_scraper/settings.py` : Configuration Scrapy

## ğŸ“š Utilisation

1. Rechercher un livre via l'interface web
2. SÃ©lectionner parmi les 3 propositions
3. Cliquer sur "TÃ©lÃ©charger EPUB"
4. Le systÃ¨me lance automatiquement le scraping et gÃ©nÃ¨re l'ebook

## âš¡ Optimisations

- RequÃªtes asynchrones pour le scraping
- Mise en cache des rÃ©sultats
- Pipeline de traitement en temps rÃ©el
- Gestion des erreurs et retry automatique
- Rate limiting pour Ã©viter les blocages
